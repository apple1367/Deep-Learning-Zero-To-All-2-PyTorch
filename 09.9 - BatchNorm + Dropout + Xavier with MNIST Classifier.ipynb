{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 9.8: About Batch Normalization with MNIST Classifier\r\n",
    "\r\n",
    "Edited By Steve Ive\r\n",
    "\r\n",
    "Here, we are going to adapt the Batch Normalization to the MNIST Classifier, and compare the capability with non-adapting model.\r\n",
    "\r\n",
    "Plus, We are going to compare various models, comparing with adapt / non-adapt models between dropout, batchnorm, weight initialization(Xavier Uniform).\r\n",
    "\r\n",
    "- Model with Batch Normalization, Dropout, Weight Initialization\r\n",
    "- Model With Batch Normalization, Dropout\r\n",
    "- Model with Batch Normalization\r\n",
    "- Model with Batch Normalization, Weight Initialization\r\n",
    "- Model with Dropout, Weight Initiailization\r\n",
    "- Model with Dropout\r\n",
    "- Model with Weight Initialization.\r\n",
    "- Model with Nothing\r\n",
    "\r\n",
    "Furthermore, we are going to plot the accuracy of the models with matplotlib.\r\n",
    "\r\n",
    "Reference from\r\n",
    "\r\n",
    "https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-09_6_mnist_batchnorm.ipynb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "import torchvision.datasets as datasets\r\n",
    "import torchvision.transforms as transforms\r\n",
    "import random\r\n",
    "\r\n",
    "import matplotlib.pylab as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "\r\n",
    "torch.manual_seed(1)\r\n",
    "random.seed(1)\r\n",
    "\r\n",
    "if device == 'cuda':\r\n",
    "    torch.cuda.manual_seed_all(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "learning_rate = 0.01\r\n",
    "training_epochs = 17\r\n",
    "batch_size = 32"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "mnist_train = datasets.MNIST(root = 'MNIST_data/',\r\n",
    "                             download = True,\r\n",
    "                             transform = transforms.ToTensor(),\r\n",
    "                             train = True)\r\n",
    "mnist_test = datasets.MNIST(root = 'MNIST_data/',\r\n",
    "                            download = True,\r\n",
    "                            transform = transforms.ToTensor(),\r\n",
    "                            train = False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset = mnist_train, shuffle = True, batch_size = batch_size, drop_last = True)\r\n",
    "test_loader = torch.utils.data.DataLoader(dataset = mnist_test, shuffle = True, batch_size = batch_size, drop_last = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "class Various_MNIST_Model(nn.Module):\r\n",
    "    def __init__(self, dropout = False, batchnorm = False, weightinit = False):\r\n",
    "        super().__init__()\r\n",
    "        self.dropout = dropout\r\n",
    "        self.batchnorm = batchnorm\r\n",
    "        self.weightinit = weightinit\r\n",
    "        self.sq = None\r\n",
    "        self.model()\r\n",
    "\r\n",
    "    def moduler(self, in_size, out_size, dropout, batchnorm):\r\n",
    "        sq = ()\r\n",
    "        sq = sq + (nn.Linear(in_size, out_size), ) #if not write \"(,)\" and just writing () makes error occur since not tuple\r\n",
    "        if batchnorm:\r\n",
    "            sq = sq + (nn.BatchNorm1d(out_size), )\r\n",
    "        sq = sq + (nn.ReLU(), )\r\n",
    "        if dropout:\r\n",
    "            sq = sq + (nn.Dropout(p = 0.3), )\r\n",
    "        return sq\r\n",
    "        \r\n",
    "    def model(self):\r\n",
    "        self.sq = nn.Sequential(\r\n",
    "            *self.moduler(784, 32, self.dropout, self.batchnorm),\r\n",
    "            *self.moduler(32, 32, self.dropout, self.batchnorm),\r\n",
    "            *self.moduler(32, 32, self.dropout, self.batchnorm),\r\n",
    "            nn.Linear(32, 10)\r\n",
    "        )\r\n",
    "        print(self.sq)\r\n",
    "        if self.weightinit:\r\n",
    "            self.weightinitializer()\r\n",
    "\r\n",
    "    def weightinitializer(self):\r\n",
    "        for layer in self.sq:\r\n",
    "            if type(layer) == nn.Linear:\r\n",
    "                torch.nn.init.xavier_uniform_(layer.weight)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return self.sq(x)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "print('Dropout + Batchnorm + weightinit')\r\n",
    "model_dropout_batchnorm_weightinit = Various_MNIST_Model(dropout = True, batchnorm = True, weightinit = True)\r\n",
    "optim_dbw = optim.Adam(model_dropout_batchnorm_weightinit.parameters(), lr = learning_rate)\r\n",
    "print('Dropout + Batchnorm')\r\n",
    "model_dropout_batchnorm = Various_MNIST_Model(dropout = True, batchnorm = True)\r\n",
    "optim_db = optim.Adam(model_dropout_batchnorm.parameters(), lr= learning_rate)\r\n",
    "print('Dropout')\r\n",
    "model_dropout = Various_MNIST_Model(dropout = True)\r\n",
    "optim_d = optim.Adam(model_dropout.parameters(), lr= learning_rate)\r\n",
    "print('Batchnorm')\r\n",
    "model_batchnorm = Various_MNIST_Model(batchnorm = True)\r\n",
    "optim_b = optim.Adam(model_batchnorm.parameters(), lr=learning_rate)\r\n",
    "print('Batchnorm + weightinit')\r\n",
    "model_batchnorm_weightinit = Various_MNIST_Model(batchnorm=True, weightinit = True)\r\n",
    "optim_bw = optim.Adam(model_batchnorm_weightinit.parameters(), lr=learning_rate)\r\n",
    "print('Dropout + weightinit')\r\n",
    "model_dropout_weightinit = Various_MNIST_Model(dropout = True, weightinit = True)\r\n",
    "optim_dw = optim.Adam(model_dropout_weightinit.parameters(), lr = learning_rate)\r\n",
    "print('WeightInit')\r\n",
    "model_weightinit = Various_MNIST_Model(weightinit = True)\r\n",
    "optim_w = optim.Adam(model_weightinit.parameters(), lr= learning_rate)\r\n",
    "print('Nothing')\r\n",
    "model_nothing = Various_MNIST_Model()\r\n",
    "optim = optim.Adam(model_nothing.parameters(), lr = learning_rate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dropout + Batchnorm + weightinit\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.3, inplace=False)\n",
      "  (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU()\n",
      "  (7): Dropout(p=0.3, inplace=False)\n",
      "  (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.3, inplace=False)\n",
      "  (12): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Dropout + Batchnorm\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Dropout(p=0.3, inplace=False)\n",
      "  (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (5): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (6): ReLU()\n",
      "  (7): Dropout(p=0.3, inplace=False)\n",
      "  (8): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (9): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (10): ReLU()\n",
      "  (11): Dropout(p=0.3, inplace=False)\n",
      "  (12): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Dropout\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.3, inplace=False)\n",
      "  (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.3, inplace=False)\n",
      "  (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.3, inplace=False)\n",
      "  (9): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Batchnorm\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Batchnorm + weightinit\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (4): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (7): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (8): ReLU()\n",
      "  (9): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Dropout + weightinit\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Dropout(p=0.3, inplace=False)\n",
      "  (3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (4): ReLU()\n",
      "  (5): Dropout(p=0.3, inplace=False)\n",
      "  (6): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (7): ReLU()\n",
      "  (8): Dropout(p=0.3, inplace=False)\n",
      "  (9): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "WeightInit\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "Nothing\n",
      "Sequential(\n",
      "  (0): Linear(in_features=784, out_features=32, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train and save loss and accuracy every epoch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_losses = []\r\n",
    "train_accs = []\r\n",
    "\r\n",
    "valid_losses = []\r\n",
    "train_accs = []\r\n",
    "\r\n",
    "train_total_batch = len(train_loader)\r\n",
    "test_tot"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('buddhalight': conda)"
  },
  "interpreter": {
   "hash": "38ed4d61829b01de31b0fe0651719916120d9f7e023a62cbbfea93b7d24a50a0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}