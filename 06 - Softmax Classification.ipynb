{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 6.1: Softmax Classification\r\n",
    "\r\n",
    "Edited By Steve Ive\r\n",
    "\r\n",
    "Reference From Seungjae Lee.\r\n",
    "\r\n",
    "https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-06_1_softmax_classification.ipynb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "source": [
    "torch.manual_seed(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23a6f9603f0>"
      ]
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Softmax\r\n",
    "\r\n",
    "Convert numbers to probabilities with softmax.\r\n",
    "\r\n",
    "$ P(class=i) = \\frac{e^i}{\\sum e^i} $"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "z = torch.FloatTensor([1, 2, 3])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch has a ```softmax``` function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "hypothesis = F.softmax(z, dim = 0)\r\n",
    "print(hypothesis)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since they are probabilities, they should add up to 1. Let's do a sanity check."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "hypothesis.sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross Entropy Loss (Low-level)\r\n",
    "\r\n",
    "For multi-class classification, we use the cross entropy loss.\r\n",
    "\r\n",
    "$ L = \\frac{1}{N} \\sum - y \\log(\\hat{y}) $\r\n",
    "\r\n",
    "where $\\hat{y}$ is the predicted probability and $y$ is the correct probability (0 or 1)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create An Arbitrary Softmax values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "z = torch.rand(3, 5, requires_grad = True)\r\n",
    "hypothesis = F.softmax(z, dim = 1)\r\n",
    "print(hypothesis)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
      "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
      "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Take a Moment!\r\n",
    "\r\n",
    "** ```TORCH.RANDINT```\r\n",
    "\r\n",
    "torch.randint(low=0, high, size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\r\n",
    "\r\n",
    "Returns a tensor filled with random integers generated uniformly between low (inclusive) and high (exclusive).\r\n",
    "\r\n",
    "The shape of the tensor is defined by the variable argument size.\r\n",
    "\r\n",
    "At below, since we set the classes as 5 upon (```z=torch.rand(3, 5, requires_grad=True)```), the first argument was set to 5.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "source": [
    "y = torch.randint(5, (3,)).long()\r\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create an Arbitrary Correct Values"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "source": [
    "y_one_hot = torch.zeros_like(hypothesis)\r\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 104
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Get Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim = 1).mean()\r\n",
    "print(cost)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cross-entropy Loss with ```torch.nn.functional```\r\n",
    "\r\n",
    "PyTorch has ```F.log_softmax()``` function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "source": [
    "#Low Level Loss with Low level logsoftmax\r\n",
    "\r\n",
    "torch.log(F.softmax(z, dim=1))\r\n",
    "print((y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean())\r\n",
    "\r\n",
    "# Low Level Loss with High Level logsoftmax\r\n",
    "\r\n",
    "F.log_softmax(z, dim=1)\r\n",
    "print((y_one_hot * -F.log_softmax(z, dim=1)).sum(dim =1).mean())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n",
      "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch also has ```F.nll_loss()``` function that computes the negative likelihood."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "source": [
    "# High Level Loss with F.nll_loss(F.log_softmax)\r\n",
    "F.nll_loss(F.log_softmax(z, dim =1), y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "PyTorch also has ```F.cross_entropy``` that combines ```F.log_softmax()``` and ```F.nll_loss()```."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "source": [
    "# High Level Loss with F.cross_entropy\r\n",
    "F.cross_entropy(z, y)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.4689, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training with Low-Level Cross Entropy Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Useless data for practice"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "source": [
    "x_train = [[1, 2, 1, 1],\r\n",
    "           [2, 1, 3, 2],\r\n",
    "           [3, 1, 3, 4],\r\n",
    "           [4, 1, 5, 5],\r\n",
    "           [1, 7, 5, 5],\r\n",
    "           [1, 2, 5, 6],\r\n",
    "           [1, 6, 6, 7],\r\n",
    "           [1, 7, 7, 7]]\r\n",
    "\r\n",
    "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\r\n",
    "x_train = torch.FloatTensor(x_train)\r\n",
    "y_train = torch.LongTensor(y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Take a Moment!\r\n",
    "\r\n",
    "**TORCH.NN.FUNCTIONAL.ONE_HOTI**\r\n",
    "\r\n",
    "torch.nn.functional.one_hot(tensor, num_classes=-1) → LongTensor\r\n",
    "\r\n",
    "Takes LongTensor with index values of shape (*) and returns a tensor of shape (*, num_classes) that have zeros everywhere except where the index of last dimension matches the corresponding value of the input tensor, in which case it will be 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "source": [
    "# Model Initialize\r\n",
    "W = torch.zeros((4, 3), requires_grad=True)\r\n",
    "b = torch.zeros(1, requires_grad=True)\r\n",
    "\r\n",
    "#set optimizer\r\n",
    "optimizer = optim.SGD([W, b], lr=0.1)\r\n",
    "\r\n",
    "nb_epochs = 10000\r\n",
    "\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    #Hypothesis\r\n",
    "    pred = F.softmax(x_train.matmul(W) + b, dim = 1)\r\n",
    "\r\n",
    "    #one-hot-encoding\r\n",
    "    y_one_hot = torch.zeros_like(pred)\r\n",
    "    y_one_hot = y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\r\n",
    "    #y_one_hot == F.one_hot(y_train)\r\n",
    "\r\n",
    "    #Cost\r\n",
    "    cost = (y_one_hot * -torch.log(pred)).sum(dim =1).mean()\r\n",
    "\r\n",
    "    #Reduce Cost\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch % 1000 == 0:\r\n",
    "        x_one_hot = F.one_hot(torch.argmax(pred, dim = 1))\r\n",
    "        accuracy = (x_one_hot == y_one_hot).float().mean()*100\r\n",
    "        print('Epoch: {}/{} Accruacy: {:6f} Cost:{:.6f}'.format(epoch, nb_epochs, accuracy, cost.item()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0/10000 Accruacy: 33.333336 Cost:1.098612\n",
      "Epoch: 1000/10000 Accruacy: 75.000000 Cost:0.585815\n",
      "Epoch: 2000/10000 Accruacy: 91.666672 Cost:0.405933\n",
      "Epoch: 3000/10000 Accruacy: 100.000000 Cost:0.245907\n",
      "Epoch: 4000/10000 Accruacy: 100.000000 Cost:0.207278\n",
      "Epoch: 5000/10000 Accruacy: 100.000000 Cost:0.178310\n",
      "Epoch: 6000/10000 Accruacy: 100.000000 Cost:0.155908\n",
      "Epoch: 7000/10000 Accruacy: 100.000000 Cost:0.138157\n",
      "Epoch: 8000/10000 Accruacy: 100.000000 Cost:0.123801\n",
      "Epoch: 9000/10000 Accruacy: 100.000000 Cost:0.111988\n",
      "Epoch: 10000/10000 Accruacy: 100.000000 Cost:0.102121\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## High-level Implementation with ```nn.Module```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "source": [
    "class SoftmaxClassifierModel(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.linear = nn.Linear(4, 3)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return self.linear(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "source": [
    "model = SoftmaxClassifierModel()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "#Set Optimizer\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\r\n",
    "\r\n",
    "nb_epochs =10000\r\n",
    "\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    #prediction\r\n",
    "    pred = model(x_train)\r\n",
    "\r\n",
    "    #cost\r\n",
    "    cost = F.cross_entropy(pred, y_train)\r\n",
    "\r\n",
    "    #reduce cost\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch % 1000 == 0:\r\n",
    "        accuracy = (F.one_hot(torch.argmax(F.log_softmax(pred, dim=1), dim=1)) == F.one_hot(y_train)).float().mean()*100\r\n",
    "        print('Epoch {:2d}/{} Accuracy: {} Cost: {:.6f}'.format(epoch, nb_epochs, accuracy, cost))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch  0/10000 Accuracy: 66.66667175292969 Cost: 0.932692\n",
      "Epoch 1000/10000 Accuracy: 83.33332824707031 Cost: 0.334209\n",
      "Epoch 2000/10000 Accuracy: 100.0 Cost: 0.173013\n",
      "Epoch 3000/10000 Accuracy: 100.0 Cost: 0.127700\n",
      "Epoch 4000/10000 Accuracy: 100.0 Cost: 0.100328\n",
      "Epoch 5000/10000 Accuracy: 100.0 Cost: 0.082227\n",
      "Epoch 6000/10000 Accuracy: 100.0 Cost: 0.069463\n",
      "Epoch 7000/10000 Accuracy: 100.0 Cost: 0.060020\n",
      "Epoch 8000/10000 Accuracy: 100.0 Cost: 0.052773\n",
      "Epoch 9000/10000 Accuracy: 100.0 Cost: 0.047048\n",
      "Epoch 10000/10000 Accuracy: 100.0 Cost: 0.042417\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('buddhalight': conda)"
  },
  "interpreter": {
   "hash": "38ed4d61829b01de31b0fe0651719916120d9f7e023a62cbbfea93b7d24a50a0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}