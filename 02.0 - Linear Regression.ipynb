{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Linear Regression\n",
    "\n",
    "Edited By Steve Ive\n",
    "\n",
    "Reference from SeungJae Lee, \n",
    "\n",
    "https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-02_linear_regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical Overview\n",
    "\n",
    "$H(x)$: How to predict for a given $x$ value.\n",
    "\n",
    "$cost(W,b)$: How well $H(x)$ predicted $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f51b314470>"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Basically the PyTorch has a NCHW fomat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "W = torch.zeros(1, requires_grad = True)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "b = torch.zeros(1, requires_grad = True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyptothesis\n",
    "\n",
    "$H(x) = Wx + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    " $ cost(W, b) = \\frac{1}{m} \\sum^m_{i=1}( H(x^{(i)}) - y^{(i)})^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "tensor([[-1.],\n",
      "        [-2.],\n",
      "        [-3.]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)\n",
    "print(hypothesis - y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [4.],\n",
      "        [9.]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print((hypothesis - y_train)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "cost.backward()#Computes the gradient of current tensor w.r.t. graph leaves.\n",
    "optimizer.step()#Performs a single optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0933], requires_grad=True)\n",
      "tensor([0.0400], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(W)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Let's check whether the hypothesis works better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1333],\n",
      "        [0.2267],\n",
      "        [0.3200]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6927, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Full Code\n",
    "\n",
    "In reality, we will be training on the dataset for multiple epochs. This can be done simply with loops.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: 0.093, b: 0.040 Cost: 4.666667\n",
      "Epoch  100/1000 W: 0.873, b: 0.289 Cost: 0.012043\n",
      "Epoch  200/1000 W: 0.900, b: 0.227 Cost: 0.007442\n",
      "Epoch  300/1000 W: 0.921, b: 0.179 Cost: 0.004598\n",
      "Epoch  400/1000 W: 0.938, b: 0.140 Cost: 0.002842\n",
      "Epoch  500/1000 W: 0.951, b: 0.110 Cost: 0.001756\n",
      "Epoch  600/1000 W: 0.962, b: 0.087 Cost: 0.001085\n",
      "Epoch  700/1000 W: 0.970, b: 0.068 Cost: 0.000670\n",
      "Epoch  800/1000 W: 0.976, b: 0.054 Cost: 0.000414\n",
      "Epoch  900/1000 W: 0.981, b: 0.042 Cost: 0.000256\n",
      "Epoch 1000/1000 W: 0.985, b: 0.033 Cost: 0.000158\n"
     ]
    }
   ],
   "source": [
    "#DATA\r\n",
    "x_train = torch.FloatTensor([[1], [2], [3]]) #텐서 선언\r\n",
    "y_train = torch.FloatTensor([[1], [2], [3]]) #텐서 선언\r\n",
    "\r\n",
    "#Model Initialization\r\n",
    "W = torch.zeros(1, requires_grad=True) #전부다 초기화\r\n",
    "b = torch.zeros(1, requires_grad=True)\r\n",
    "\r\n",
    "#torch.zeros문법 : torch.zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) ==> 텐서를 반환함.\r\n",
    "# 필수 인자\r\n",
    "# size (int...) – a sequence of integers defining the shape of the output tensor. Can be a variable number of arguments or a collection like a list or tuple.\r\n",
    "\r\n",
    "# 선택 인자들\r\n",
    "# out (Tensor, optional) – 어느 텐서에 작업을 수행할지\r\n",
    "# dtype (torch.dtype, optional) – 반환될 텐서의 자료형. 만약에 없으면, 전역 기본값을 사용함 (see torch.set_default_tensor_type()).\r\n",
    "# layout (torch.layout, optional) – 반환될 텐서의 레이아웃 Default: torch.strided.\r\n",
    "# device (torch.device, optional) – 반환될 텐서의 디바이스 Default: if None, uses the current device for the default tensor type (see torch.set_default_tensor_type()). device will be the CPU for CPU tensor types and the current CUDA device for CUDA tensor types.\r\n",
    "# requires_grad (bool, optional) – autograd가 텐서의 변화를 자동으로 기억할지 (기본값 = off)\r\n",
    "\r\n",
    "#Set Optimizer\r\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\r\n",
    "# 경사하강법등을 해주는 함수. optim.SGD말고도 여러 함수가 있으나 지금은 SGD를 사용\r\n",
    "\r\n",
    "nb_epochs = 1000\r\n",
    "# 반복 수행할 횟수 선언\r\n",
    "\r\n",
    "for epoch in range(nb_epochs + 1): #에포치 번 반복\r\n",
    "\r\n",
    "    hypothesis = x_train * W + b #가설 설정. h(x) = Wx + b\r\n",
    "\r\n",
    "    cost = torch.mean((hypothesis - y_train)**2) #cost function 정의\r\n",
    "\r\n",
    "    optimizer.zero_grad() #pytorch는 미분된값을 기존 optimizer에 계속 누적시키므로, 0으로 초기화해야함\r\n",
    "    cost.backward() #cost 함수의 역전파\r\n",
    "    optimizer.step() #optimizer으로 weight들을 조정해줌\r\n",
    "\r\n",
    "    #print logs for 100 epochs\r\n",
    "    if epoch % 100 == 0: #100번의 수행마다\r\n",
    "        print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(epoch, nb_epochs, W.item(), b.item(), cost.item())) \r\n",
    "        #이런 출력을 한다. (중요한건 아님)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0186],\n",
      "        [2.0040],\n",
      "        [2.9894]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High-level Implementation with ```nn.Module```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basically, all the models providing by PyTorch are made of inheriting ```nn.Module```. Now we are going to build linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.linear = nn.Linear(1, 1)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At the model __init__, we are going to define the layers that will be used. Here, we are building the linear regression model, we will use ```nn.Linear```. And at the ```forward```, we will tell it how this model should return the output from the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "Now Let's make hypothesis by generating model\n",
    "\n",
    "Basically, The ***Hypothesis*** means the return value of ```forward()``` of ```nn.Module```, which means that it is the forward result of the Linear Layer.\n",
    "\n",
    "### **Hypothesis === Forward**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "hypothesis = model(x_train)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0739],\n",
      "        [0.5891],\n",
      "        [1.1044]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost\n",
    "\n",
    "Now, Let's get cost by MSE(Mean Squared Error). MSE function is also provided by PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0739],\n",
      "        [0.5891],\n",
      "        [1.1044]], grad_fn=<AddmmBackward>)\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = F.mse_loss(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1471, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Let's reduce the cost by the optimizer providing by PyTorch. You can use one of the optimizers in ```torch.optim```. Here, we will use the SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "cost.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Full Code\n",
    "\n",
    "Now, we understand the Linear Regression, Let's fit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 W: -0.114, b:  0.546798 Cost:  4.589475\n",
      "Epoch  100/1000 W:  0.700, b:  0.682770 Cost:  0.067199\n",
      "Epoch  200/1000 W:  0.764, b:  0.536722 Cost:  0.041525\n",
      "Epoch  300/1000 W:  0.814, b:  0.421912 Cost:  0.025660\n",
      "Epoch  400/1000 W:  0.854, b:  0.331661 Cost:  0.015856\n",
      "Epoch  500/1000 W:  0.885, b:  0.260716 Cost:  0.009798\n",
      "Epoch  600/1000 W:  0.910, b:  0.204947 Cost:  0.006055\n",
      "Epoch  700/1000 W:  0.929, b:  0.161107 Cost:  0.003741\n",
      "Epoch  800/1000 W:  0.944, b:  0.126644 Cost:  0.002312\n",
      "Epoch  900/1000 W:  0.956, b:  0.099554 Cost:  0.001429\n",
      "Epoch 1000/1000 W:  0.966, b:  0.078259 Cost:  0.000883\n"
     ]
    }
   ],
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim\r\n",
    "\r\n",
    "# Data\r\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\r\n",
    "y_train = torch.FloatTensor([[1], [2], [3]])\r\n",
    "\r\n",
    "class LinearRegressionModel(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.linear = nn.Linear(1, 1)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return self.linear(x)\r\n",
    "\r\n",
    "# 왜 클래스로써 nn을 정의하는지 자세한 설명은 https://wikidocs.net/60036 에 있습니다\r\n",
    "\r\n",
    "#Initialize the Model\r\n",
    "model = LinearRegressionModel() \r\n",
    "# linear이 포함된 클래스로 모델을 정의합니다\r\n",
    "\r\n",
    "#Set the optimizer\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\r\n",
    "\r\n",
    "nb_epochs = 1000\r\n",
    "\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    #Hypothsis\r\n",
    "    pred = model(x_train)\r\n",
    "\r\n",
    "    #Cost\r\n",
    "    cost = F.mse_loss(pred, y_train)\r\n",
    "    # 파이토치에서 제공하는 평균 제곱 오차 함수입니다. (mean squared error)\r\n",
    "\r\n",
    "    #Optimize the Cost\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    #Print Logs for 100 epochs\r\n",
    "    if epoch % 100 == 0:\r\n",
    "        params = list(model.parameters())\r\n",
    "        W = params[0].item()\r\n",
    "        b = params[1].item()\r\n",
    "        print('Epoch {:4d}/{} W: {: .3f}, b: {: 3f} Cost: {: .6f}'.format(\r\n",
    "            epoch, nb_epochs, W, b, cost.item()\r\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37cc7290eaadfd237ba9edfee5f5646f2226d20dfbf33e67928708333f7a6cd3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('deep-learning': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}