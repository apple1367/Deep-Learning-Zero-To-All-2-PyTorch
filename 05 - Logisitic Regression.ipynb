{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('buddhalight': conda)"
  },
  "interpreter": {
   "hash": "38ed4d61829b01de31b0fe0651719916120d9f7e023a62cbbfea93b7d24a50a0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 5: Logistic Regression\n",
    "\n",
    "Edited By Steve Ive\n",
    "\n",
    "Reference From Seungjae Lee\n",
    "\n",
    "https://github.com/deeplearningzerotoall/PyTorch/blob/master/lab-05_logistic_classification.ipynb"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Reminder: Logistic Regression\n",
    "\n",
    "### Hypothesis\n",
    "\n",
    "$ H(X) = \\frac{1}{1+e^{-W^T X}} $\n",
    "\n",
    "### Cost\n",
    "\n",
    "$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $\n",
    "\n",
    "- If $y \\simeq H(x)$, cost is near 0.\n",
    "\n",
    "- If $y \\neq H(x)$, cost is high.\n",
    "\n",
    "### Weight Update via Gradient Descent\n",
    "\n",
    "$ W := W - \\alpha \\frac{\\partial}{\\partial W} cost(W) $\n",
    "\n",
    "- $\\alpha$: Learning rate"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.optim as optim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "source": [
    "torch.manual_seed(1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22db807b450>"
      ]
     },
     "metadata": {},
     "execution_count": 264
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\r\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Consider the following classification problem: given the number of hours each student spent watching the lecture and working in the code lab, predict whether the student passed or failed a course. For example, the first (index 0) student watched the lecture for 1 hour and spent 2 hours in the lab session ```([1, 2])```, and ended up failing the course ```([0])```."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "source": [
    "x_train = torch.FloatTensor(x_data)\r\n",
    "y_train = torch.FloatTensor(y_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As always, we need these data to be in torch.Tensor format, so we convert them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "source": [
    "print(x_train.shape)\r\n",
    "print(y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing the Hypothesis\n",
    "\n",
    "$ H(X) = \\frac{1}{1+e^{-W^T X}} $\n",
    "\n",
    "PyTorch has a torch.exp() function that resembles the exponential function."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "source": [
    "print('e^1 equals: ', torch.exp(torch.FloatTensor([1])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e^1 equals:  tensor([2.7183])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use it to compute the hypothesis function conveniently."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "source": [
    "W = torch.zeros((2, 1), requires_grad=True)\r\n",
    "b = torch.zeros(1, requires_grad = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "source": [
    "print(hypothesis)\r\n",
    "print(hypothesis.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Or, we could use torch.sigmoid() function! This resembles the sigmoid function:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "source": [
    "print('1/(1+e^{-1}) equals: ', torch.sigmoid(torch.FloatTensor([1])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1/(1+e^{-1}) equals:  tensor([0.7311])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, the code for hypothesis function is cleaner."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "source": [
    "print(hypothesis)\r\n",
    "print(hypothesis.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing the Cost Function (Low-level)\n",
    "\n",
    "$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $\n",
    "\n",
    "We want to measure the difference between hypothesis and y_train."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "source": [
    "print(hypothesis)\r\n",
    "print(y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "For one element, the loss can be computed as follows:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "source": [
    "-(y_train[0] * torch.log(hypothesis[0]) + (1 - y_train[0]) * torch.log(1 - hypothesis[0]))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.6931], grad_fn=<NegBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 276
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To compute the losses for the entire batch, we can simply input the entire vector."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "source": [
    "losses = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis))\r\n",
    "\r\n",
    "print(losses)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931]], grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we just .mean() to take the mean of these individual losses."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "source": [
    "cost = losses.mean()\r\n",
    "print(cost)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Computing the Cost Function with ```F.binary_cross_entropy```\n",
    "\n",
    "In reality, binary classification is used so often that PyTorch has a simple function called F.binary_cross_entropy implemented to lighten the burden."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "source": [
    "F.binary_cross_entropy(hypothesis, y_train)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 279
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training with Low-level Binary Cross Entorpy Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\r\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\r\n",
    "x_train = torch.FloatTensor(x_data)\r\n",
    "y_train = torch.FloatTensor(y_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "source": [
    "#Model Initialize\r\n",
    "W = torch.zeros((2, 1), requires_grad=True)\r\n",
    "b = torch.zeros(1, requires_grad=True)\r\n",
    "\r\n",
    "#Set Optimizer\r\n",
    "optimizer = optim.SGD([W, b], lr=1)\r\n",
    "\r\n",
    "nb_epochs = 1000\r\n",
    "\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    #Hypothesis\r\n",
    "    pred = torch.sigmoid(x_train.matmul(W) + b)\r\n",
    "\r\n",
    "    #Cost\r\n",
    "    cost = -(y_train * torch.log(pred) + (1 - y_train) * torch.log(1 - pred)).mean()\r\n",
    "\r\n",
    "    #Reduce Cost\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch % 100 == 0:\r\n",
    "        print('Epoch {:4d}/{} \\n Hypothesis: {} \\n Cost: {:.6f}'.format(epoch, nb_epochs, pred.squeeze().detach(), cost.item()))\r\n",
    "        print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 \n",
      " Hypothesis: tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]) \n",
      " Cost: 0.693147\n",
      "\n",
      "Epoch  100/1000 \n",
      " Hypothesis: tensor([0.0245, 0.1484, 0.2770, 0.7954, 0.9484, 0.9834]) \n",
      " Cost: 0.134722\n",
      "\n",
      "Epoch  200/1000 \n",
      " Hypothesis: tensor([0.0080, 0.1065, 0.1632, 0.8566, 0.9769, 0.9931]) \n",
      " Cost: 0.080643\n",
      "\n",
      "Epoch  300/1000 \n",
      " Hypothesis: tensor([0.0037, 0.0822, 0.1161, 0.8888, 0.9869, 0.9965]) \n",
      " Cost: 0.057900\n",
      "\n",
      "Epoch  400/1000 \n",
      " Hypothesis: tensor([0.0021, 0.0669, 0.0902, 0.9090, 0.9916, 0.9979]) \n",
      " Cost: 0.045300\n",
      "\n",
      "Epoch  500/1000 \n",
      " Hypothesis: tensor([0.0013, 0.0564, 0.0739, 0.9229, 0.9941, 0.9986]) \n",
      " Cost: 0.037261\n",
      "\n",
      "Epoch  600/1000 \n",
      " Hypothesis: tensor([8.7256e-04, 4.8759e-02, 6.2629e-02, 9.3312e-01, 9.9567e-01, 9.9906e-01]) \n",
      " Cost: 0.031673\n",
      "\n",
      "Epoch  700/1000 \n",
      " Hypothesis: tensor([6.2087e-04, 4.2945e-02, 5.4368e-02, 9.4091e-01, 9.9668e-01, 9.9932e-01]) \n",
      " Cost: 0.027556\n",
      "\n",
      "Epoch  800/1000 \n",
      " Hypothesis: tensor([4.6039e-04, 3.8371e-02, 4.8050e-02, 9.4706e-01, 9.9737e-01, 9.9949e-01]) \n",
      " Cost: 0.024394\n",
      "\n",
      "Epoch  900/1000 \n",
      " Hypothesis: tensor([3.5258e-04, 3.4679e-02, 4.3059e-02, 9.5205e-01, 9.9786e-01, 9.9961e-01]) \n",
      " Cost: 0.021888\n",
      "\n",
      "Epoch 1000/1000 \n",
      " Hypothesis: tensor([2.7711e-04, 3.1636e-02, 3.9014e-02, 9.5618e-01, 9.9823e-01, 9.9969e-01]) \n",
      " Cost: 0.019852\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training with ```F.binary_cross_entropy```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "source": [
    "#Model Initialize\r\n",
    "W = torch.zeros((2, 1), requires_grad = True)\r\n",
    "b = torch.zeros(1, requires_grad = True)\r\n",
    "\r\n",
    "#Set Optimizer\r\n",
    "optimizer = optim.SGD([W, b], lr=1)\r\n",
    "\r\n",
    "nb_epochs = 1000\r\n",
    "\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    #Hypothesis\r\n",
    "    pred = torch.sigmoid(x_train.matmul(W) + b)\r\n",
    "\r\n",
    "    #Cost Function\r\n",
    "    cost = F.binary_cross_entropy(pred, y_train)\r\n",
    "\r\n",
    "    #Reduce Cost\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch % 100 == 0:\r\n",
    "        print('Epoch {:4d}/{} \\n Hypothesis: {} \\n Cost: {:.6f}'.format(epoch, nb_epochs, pred.squeeze().detach(), cost))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 \n",
      " Hypothesis: tensor([0.5000, 0.5000, 0.5000, 0.5000, 0.5000, 0.5000]) \n",
      " Cost: 0.693147\n",
      "Epoch  100/1000 \n",
      " Hypothesis: tensor([0.0245, 0.1484, 0.2770, 0.7954, 0.9484, 0.9834]) \n",
      " Cost: 0.134722\n",
      "Epoch  200/1000 \n",
      " Hypothesis: tensor([0.0080, 0.1065, 0.1632, 0.8566, 0.9769, 0.9931]) \n",
      " Cost: 0.080643\n",
      "Epoch  300/1000 \n",
      " Hypothesis: tensor([0.0037, 0.0822, 0.1161, 0.8888, 0.9869, 0.9965]) \n",
      " Cost: 0.057900\n",
      "Epoch  400/1000 \n",
      " Hypothesis: tensor([0.0021, 0.0669, 0.0902, 0.9090, 0.9916, 0.9979]) \n",
      " Cost: 0.045300\n",
      "Epoch  500/1000 \n",
      " Hypothesis: tensor([0.0013, 0.0564, 0.0739, 0.9229, 0.9941, 0.9986]) \n",
      " Cost: 0.037261\n",
      "Epoch  600/1000 \n",
      " Hypothesis: tensor([8.7256e-04, 4.8759e-02, 6.2629e-02, 9.3312e-01, 9.9567e-01, 9.9906e-01]) \n",
      " Cost: 0.031673\n",
      "Epoch  700/1000 \n",
      " Hypothesis: tensor([6.2087e-04, 4.2945e-02, 5.4368e-02, 9.4091e-01, 9.9668e-01, 9.9932e-01]) \n",
      " Cost: 0.027556\n",
      "Epoch  800/1000 \n",
      " Hypothesis: tensor([4.6039e-04, 3.8371e-02, 4.8050e-02, 9.4706e-01, 9.9737e-01, 9.9949e-01]) \n",
      " Cost: 0.024394\n",
      "Epoch  900/1000 \n",
      " Hypothesis: tensor([3.5258e-04, 3.4679e-02, 4.3059e-02, 9.5205e-01, 9.9786e-01, 9.9961e-01]) \n",
      " Cost: 0.021888\n",
      "Epoch 1000/1000 \n",
      " Hypothesis: tensor([2.7711e-04, 3.1636e-02, 3.9014e-02, 9.5618e-01, 9.9823e-01, 9.9969e-01]) \n",
      " Cost: 0.019852\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading Real Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "source": [
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\r\n",
    "x_data = xy[:, 0:-1]\r\n",
    "y_data = xy[:, [-1]]\r\n",
    "\r\n",
    "x_train = torch.FloatTensor(x_data)\r\n",
    "y_train = torch.FloatTensor(y_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "source": [
    "print(x_train.shape)\r\n",
    "print(y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "source": [
    "print(x_train[:5])\r\n",
    "print(y_train[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n",
      "        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n",
      "        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n",
      "        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n",
      "        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training with Real Data using low-level Binary Cross Entropy Loss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "source": [
    "#Model Initialize\r\n",
    "W = torch.zeros((8, 1), requires_grad=True)\r\n",
    "b = torch.zeros(1, requires_grad=True)\r\n",
    "\r\n",
    "#Set optimizer\r\n",
    "optimizer = optim.SGD([W, b], lr=1)\r\n",
    "\r\n",
    "nb_epochs = 1000\r\n",
    "\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    #Hypothesis\r\n",
    "    pred = torch.sigmoid(x_train.matmul(W) + b)\r\n",
    "\r\n",
    "    #Cost Function\r\n",
    "    cost = -(y_train * torch.log(pred) + (1 - y_train) * torch.log(1 - pred)).mean()\r\n",
    "\r\n",
    "    #Reduce Cost\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch % 10 == 0:\r\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch   10/1000 Cost: 0.572727\n",
      "Epoch   20/1000 Cost: 0.539493\n",
      "Epoch   30/1000 Cost: 0.519708\n",
      "Epoch   40/1000 Cost: 0.507066\n",
      "Epoch   50/1000 Cost: 0.498539\n",
      "Epoch   60/1000 Cost: 0.492549\n",
      "Epoch   70/1000 Cost: 0.488209\n",
      "Epoch   80/1000 Cost: 0.484985\n",
      "Epoch   90/1000 Cost: 0.482543\n",
      "Epoch  100/1000 Cost: 0.480661\n",
      "Epoch  110/1000 Cost: 0.479189\n",
      "Epoch  120/1000 Cost: 0.478023\n",
      "Epoch  130/1000 Cost: 0.477088\n",
      "Epoch  140/1000 Cost: 0.476331\n",
      "Epoch  150/1000 Cost: 0.475711\n",
      "Epoch  160/1000 Cost: 0.475198\n",
      "Epoch  170/1000 Cost: 0.474771\n",
      "Epoch  180/1000 Cost: 0.474411\n",
      "Epoch  190/1000 Cost: 0.474107\n",
      "Epoch  200/1000 Cost: 0.473846\n",
      "Epoch  210/1000 Cost: 0.473622\n",
      "Epoch  220/1000 Cost: 0.473428\n",
      "Epoch  230/1000 Cost: 0.473259\n",
      "Epoch  240/1000 Cost: 0.473111\n",
      "Epoch  250/1000 Cost: 0.472980\n",
      "Epoch  260/1000 Cost: 0.472864\n",
      "Epoch  270/1000 Cost: 0.472761\n",
      "Epoch  280/1000 Cost: 0.472669\n",
      "Epoch  290/1000 Cost: 0.472586\n",
      "Epoch  300/1000 Cost: 0.472511\n",
      "Epoch  310/1000 Cost: 0.472444\n",
      "Epoch  320/1000 Cost: 0.472383\n",
      "Epoch  330/1000 Cost: 0.472327\n",
      "Epoch  340/1000 Cost: 0.472277\n",
      "Epoch  350/1000 Cost: 0.472230\n",
      "Epoch  360/1000 Cost: 0.472188\n",
      "Epoch  370/1000 Cost: 0.472149\n",
      "Epoch  380/1000 Cost: 0.472114\n",
      "Epoch  390/1000 Cost: 0.472081\n",
      "Epoch  400/1000 Cost: 0.472051\n",
      "Epoch  410/1000 Cost: 0.472023\n",
      "Epoch  420/1000 Cost: 0.471997\n",
      "Epoch  430/1000 Cost: 0.471974\n",
      "Epoch  440/1000 Cost: 0.471952\n",
      "Epoch  450/1000 Cost: 0.471932\n",
      "Epoch  460/1000 Cost: 0.471913\n",
      "Epoch  470/1000 Cost: 0.471896\n",
      "Epoch  480/1000 Cost: 0.471880\n",
      "Epoch  490/1000 Cost: 0.471865\n",
      "Epoch  500/1000 Cost: 0.471851\n",
      "Epoch  510/1000 Cost: 0.471839\n",
      "Epoch  520/1000 Cost: 0.471827\n",
      "Epoch  530/1000 Cost: 0.471816\n",
      "Epoch  540/1000 Cost: 0.471806\n",
      "Epoch  550/1000 Cost: 0.471796\n",
      "Epoch  560/1000 Cost: 0.471787\n",
      "Epoch  570/1000 Cost: 0.471779\n",
      "Epoch  580/1000 Cost: 0.471772\n",
      "Epoch  590/1000 Cost: 0.471765\n",
      "Epoch  600/1000 Cost: 0.471758\n",
      "Epoch  610/1000 Cost: 0.471752\n",
      "Epoch  620/1000 Cost: 0.471746\n",
      "Epoch  630/1000 Cost: 0.471741\n",
      "Epoch  640/1000 Cost: 0.471736\n",
      "Epoch  650/1000 Cost: 0.471732\n",
      "Epoch  660/1000 Cost: 0.471728\n",
      "Epoch  670/1000 Cost: 0.471724\n",
      "Epoch  680/1000 Cost: 0.471720\n",
      "Epoch  690/1000 Cost: 0.471717\n",
      "Epoch  700/1000 Cost: 0.471713\n",
      "Epoch  710/1000 Cost: 0.471710\n",
      "Epoch  720/1000 Cost: 0.471708\n",
      "Epoch  730/1000 Cost: 0.471705\n",
      "Epoch  740/1000 Cost: 0.471703\n",
      "Epoch  750/1000 Cost: 0.471701\n",
      "Epoch  760/1000 Cost: 0.471698\n",
      "Epoch  770/1000 Cost: 0.471697\n",
      "Epoch  780/1000 Cost: 0.471695\n",
      "Epoch  790/1000 Cost: 0.471693\n",
      "Epoch  800/1000 Cost: 0.471692\n",
      "Epoch  810/1000 Cost: 0.471690\n",
      "Epoch  820/1000 Cost: 0.471689\n",
      "Epoch  830/1000 Cost: 0.471688\n",
      "Epoch  840/1000 Cost: 0.471686\n",
      "Epoch  850/1000 Cost: 0.471685\n",
      "Epoch  860/1000 Cost: 0.471684\n",
      "Epoch  870/1000 Cost: 0.471683\n",
      "Epoch  880/1000 Cost: 0.471683\n",
      "Epoch  890/1000 Cost: 0.471682\n",
      "Epoch  900/1000 Cost: 0.471681\n",
      "Epoch  910/1000 Cost: 0.471680\n",
      "Epoch  920/1000 Cost: 0.471680\n",
      "Epoch  930/1000 Cost: 0.471679\n",
      "Epoch  940/1000 Cost: 0.471678\n",
      "Epoch  950/1000 Cost: 0.471678\n",
      "Epoch  960/1000 Cost: 0.471677\n",
      "Epoch  970/1000 Cost: 0.471677\n",
      "Epoch  980/1000 Cost: 0.471677\n",
      "Epoch  990/1000 Cost: 0.471676\n",
      "Epoch 1000/1000 Cost: 0.471676\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training with Real Data using ```F.binary_cross_entropy```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "source": [
    "# Model Initialize\r\n",
    "W = torch.zeros((8, 1), requires_grad=True)\r\n",
    "b = torch.zeros(1, requires_grad=True)\r\n",
    "\r\n",
    "#Set Optimizer\r\n",
    "optimizer = optim.SGD([W, b], lr=1)\r\n",
    "\r\n",
    "nb_epochs = 1000\r\n",
    "\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    #Hypothesis\r\n",
    "    pred = torch.sigmoid(x_train.matmul(W) + b)\r\n",
    "\r\n",
    "    #Cost\r\n",
    "    cost = F.binary_cross_entropy(pred, y_train)\r\n",
    "\r\n",
    "    #Reduce Cost\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch % 10 == 0:\r\n",
    "        print('Epoch {:4d}/{} \\n Cost: {:.6f}'.format(epoch, nb_epochs, cost.item()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/1000 \n",
      " Cost: 0.693147\n",
      "Epoch   10/1000 \n",
      " Cost: 0.572727\n",
      "Epoch   20/1000 \n",
      " Cost: 0.539493\n",
      "Epoch   30/1000 \n",
      " Cost: 0.519708\n",
      "Epoch   40/1000 \n",
      " Cost: 0.507066\n",
      "Epoch   50/1000 \n",
      " Cost: 0.498539\n",
      "Epoch   60/1000 \n",
      " Cost: 0.492549\n",
      "Epoch   70/1000 \n",
      " Cost: 0.488209\n",
      "Epoch   80/1000 \n",
      " Cost: 0.484985\n",
      "Epoch   90/1000 \n",
      " Cost: 0.482543\n",
      "Epoch  100/1000 \n",
      " Cost: 0.480661\n",
      "Epoch  110/1000 \n",
      " Cost: 0.479189\n",
      "Epoch  120/1000 \n",
      " Cost: 0.478023\n",
      "Epoch  130/1000 \n",
      " Cost: 0.477088\n",
      "Epoch  140/1000 \n",
      " Cost: 0.476331\n",
      "Epoch  150/1000 \n",
      " Cost: 0.475711\n",
      "Epoch  160/1000 \n",
      " Cost: 0.475198\n",
      "Epoch  170/1000 \n",
      " Cost: 0.474771\n",
      "Epoch  180/1000 \n",
      " Cost: 0.474411\n",
      "Epoch  190/1000 \n",
      " Cost: 0.474107\n",
      "Epoch  200/1000 \n",
      " Cost: 0.473846\n",
      "Epoch  210/1000 \n",
      " Cost: 0.473622\n",
      "Epoch  220/1000 \n",
      " Cost: 0.473428\n",
      "Epoch  230/1000 \n",
      " Cost: 0.473259\n",
      "Epoch  240/1000 \n",
      " Cost: 0.473111\n",
      "Epoch  250/1000 \n",
      " Cost: 0.472980\n",
      "Epoch  260/1000 \n",
      " Cost: 0.472864\n",
      "Epoch  270/1000 \n",
      " Cost: 0.472761\n",
      "Epoch  280/1000 \n",
      " Cost: 0.472669\n",
      "Epoch  290/1000 \n",
      " Cost: 0.472586\n",
      "Epoch  300/1000 \n",
      " Cost: 0.472511\n",
      "Epoch  310/1000 \n",
      " Cost: 0.472444\n",
      "Epoch  320/1000 \n",
      " Cost: 0.472383\n",
      "Epoch  330/1000 \n",
      " Cost: 0.472327\n",
      "Epoch  340/1000 \n",
      " Cost: 0.472277\n",
      "Epoch  350/1000 \n",
      " Cost: 0.472230\n",
      "Epoch  360/1000 \n",
      " Cost: 0.472188\n",
      "Epoch  370/1000 \n",
      " Cost: 0.472149\n",
      "Epoch  380/1000 \n",
      " Cost: 0.472114\n",
      "Epoch  390/1000 \n",
      " Cost: 0.472081\n",
      "Epoch  400/1000 \n",
      " Cost: 0.472051\n",
      "Epoch  410/1000 \n",
      " Cost: 0.472023\n",
      "Epoch  420/1000 \n",
      " Cost: 0.471997\n",
      "Epoch  430/1000 \n",
      " Cost: 0.471974\n",
      "Epoch  440/1000 \n",
      " Cost: 0.471952\n",
      "Epoch  450/1000 \n",
      " Cost: 0.471932\n",
      "Epoch  460/1000 \n",
      " Cost: 0.471913\n",
      "Epoch  470/1000 \n",
      " Cost: 0.471896\n",
      "Epoch  480/1000 \n",
      " Cost: 0.471880\n",
      "Epoch  490/1000 \n",
      " Cost: 0.471865\n",
      "Epoch  500/1000 \n",
      " Cost: 0.471851\n",
      "Epoch  510/1000 \n",
      " Cost: 0.471839\n",
      "Epoch  520/1000 \n",
      " Cost: 0.471827\n",
      "Epoch  530/1000 \n",
      " Cost: 0.471816\n",
      "Epoch  540/1000 \n",
      " Cost: 0.471806\n",
      "Epoch  550/1000 \n",
      " Cost: 0.471796\n",
      "Epoch  560/1000 \n",
      " Cost: 0.471787\n",
      "Epoch  570/1000 \n",
      " Cost: 0.471779\n",
      "Epoch  580/1000 \n",
      " Cost: 0.471772\n",
      "Epoch  590/1000 \n",
      " Cost: 0.471765\n",
      "Epoch  600/1000 \n",
      " Cost: 0.471758\n",
      "Epoch  610/1000 \n",
      " Cost: 0.471752\n",
      "Epoch  620/1000 \n",
      " Cost: 0.471746\n",
      "Epoch  630/1000 \n",
      " Cost: 0.471741\n",
      "Epoch  640/1000 \n",
      " Cost: 0.471736\n",
      "Epoch  650/1000 \n",
      " Cost: 0.471732\n",
      "Epoch  660/1000 \n",
      " Cost: 0.471728\n",
      "Epoch  670/1000 \n",
      " Cost: 0.471724\n",
      "Epoch  680/1000 \n",
      " Cost: 0.471720\n",
      "Epoch  690/1000 \n",
      " Cost: 0.471717\n",
      "Epoch  700/1000 \n",
      " Cost: 0.471713\n",
      "Epoch  710/1000 \n",
      " Cost: 0.471710\n",
      "Epoch  720/1000 \n",
      " Cost: 0.471708\n",
      "Epoch  730/1000 \n",
      " Cost: 0.471705\n",
      "Epoch  740/1000 \n",
      " Cost: 0.471703\n",
      "Epoch  750/1000 \n",
      " Cost: 0.471701\n",
      "Epoch  760/1000 \n",
      " Cost: 0.471698\n",
      "Epoch  770/1000 \n",
      " Cost: 0.471697\n",
      "Epoch  780/1000 \n",
      " Cost: 0.471695\n",
      "Epoch  790/1000 \n",
      " Cost: 0.471693\n",
      "Epoch  800/1000 \n",
      " Cost: 0.471692\n",
      "Epoch  810/1000 \n",
      " Cost: 0.471690\n",
      "Epoch  820/1000 \n",
      " Cost: 0.471689\n",
      "Epoch  830/1000 \n",
      " Cost: 0.471688\n",
      "Epoch  840/1000 \n",
      " Cost: 0.471686\n",
      "Epoch  850/1000 \n",
      " Cost: 0.471685\n",
      "Epoch  860/1000 \n",
      " Cost: 0.471684\n",
      "Epoch  870/1000 \n",
      " Cost: 0.471683\n",
      "Epoch  880/1000 \n",
      " Cost: 0.471683\n",
      "Epoch  890/1000 \n",
      " Cost: 0.471682\n",
      "Epoch  900/1000 \n",
      " Cost: 0.471681\n",
      "Epoch  910/1000 \n",
      " Cost: 0.471680\n",
      "Epoch  920/1000 \n",
      " Cost: 0.471680\n",
      "Epoch  930/1000 \n",
      " Cost: 0.471679\n",
      "Epoch  940/1000 \n",
      " Cost: 0.471678\n",
      "Epoch  950/1000 \n",
      " Cost: 0.471678\n",
      "Epoch  960/1000 \n",
      " Cost: 0.471677\n",
      "Epoch  970/1000 \n",
      " Cost: 0.471677\n",
      "Epoch  980/1000 \n",
      " Cost: 0.471677\n",
      "Epoch  990/1000 \n",
      " Cost: 0.471676\n",
      "Epoch 1000/1000 \n",
      " Cost: 0.471676\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking the Accuracy our Model\n",
    "\n",
    "After we finish training the model, we want to check how well our model fits the training set."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\r\n",
    "print(hypothesis[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.3555],\n",
      "        [0.9564],\n",
      "        [0.1934],\n",
      "        [0.9623],\n",
      "        [0.0706]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can change hypothesis (real number from 0 to 1) to binary predictions (either 0 or 1) by comparing them to 0.5."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\r\n",
    "print(prediction[:5].float())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, we compare it with the correct labels y_train."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "source": [
    "print(prediction[:5].float())\r\n",
    "print(y_train[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "source": [
    "correct_prediction = prediction.float() == y_train\r\n",
    "print(correct_prediction[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can calculate the accuracy by counting the number of correct predictions and dividng by total number of predictions."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Take a Moment!\n",
    "\n",
    "Why we attach tensor.item() ?\n",
    "\n",
    "if just print(tensor) => tensor(value)\n",
    "else if print(tensor.item()) => value"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "source": [
    "accuracy = correct_prediction.sum().item() / len(correct_prediction)\r\n",
    "\r\n",
    "print('The model has an accuracy of {:2.2f}% for the training set.'.format(accuracy * 100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The model has an accuracy of 76.94% for the training set.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optional: High-level Implementation with ```nn.Module```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "source": [
    "class BinaryClassfier(nn.Module):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        self.linear = nn.Linear(8, 1)\r\n",
    "        self.sigmoid = nn.Sigmoid()\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return self.sigmoid(self.linear(x))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "source": [
    "model = BinaryClassfier()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "source": [
    "#Set Optimizer\r\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\r\n",
    "\r\n",
    "nb_epochs = 100\r\n",
    "\r\n",
    "for epoch in range(nb_epochs + 1):\r\n",
    "\r\n",
    "    #Hypothesis\r\n",
    "    pred = model(x_train)\r\n",
    "\r\n",
    "    #Cost\r\n",
    "    cost = F.binary_cross_entropy(pred, y_train)\r\n",
    "\r\n",
    "    #Reduce Cost\r\n",
    "    optimizer.zero_grad()\r\n",
    "    cost.backward()\r\n",
    "    optimizer.step()\r\n",
    "\r\n",
    "    if epoch % 10 == 0:\r\n",
    "        prediction = pred >= torch.FloatTensor([0.5])\r\n",
    "        correct_prediction = prediction.float() == y_train\r\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\r\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy: {:2.2f}'.format(epoch, nb_epochs, cost.item(), accuracy * 100))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch    0/100 Cost: 0.704829 Accuracy: 45.72\n",
      "Epoch   10/100 Cost: 0.572392 Accuracy: 67.59\n",
      "Epoch   20/100 Cost: 0.539563 Accuracy: 73.25\n",
      "Epoch   30/100 Cost: 0.520041 Accuracy: 75.89\n",
      "Epoch   40/100 Cost: 0.507561 Accuracy: 76.15\n",
      "Epoch   50/100 Cost: 0.499125 Accuracy: 76.42\n",
      "Epoch   60/100 Cost: 0.493177 Accuracy: 77.21\n",
      "Epoch   70/100 Cost: 0.488846 Accuracy: 76.81\n",
      "Epoch   80/100 Cost: 0.485612 Accuracy: 76.28\n",
      "Epoch   90/100 Cost: 0.483146 Accuracy: 76.55\n",
      "Epoch  100/100 Cost: 0.481234 Accuracy: 76.81\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}